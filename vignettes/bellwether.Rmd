---
title: "Bellwether Tutorial"
author: "Josh Sumner, DDPSC Data Science Core Facility"
subtitle: "pcvr v0.1.0"
output: 
  html_document:
    toc: true
    number_sections: false
    code_folding: show
date: '2023-05-30'
vignette: >
  %\VignetteIndexEntry{pcvr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(pcvr) # devtools::load_all()
library(data.table) # for fread
library(ggplot2)
library(patchwork) # for easy ggplot manipulation/combination
```


# Example Bellwether (Lemnatech) Workflow

The Bellwether phenotyping facility at the Donald Danforth Plant Science Center allows for high throughput image based phenotyping of up to 1140 plants over the course of several weeks. This generates a massive amount of image data which is typically analysed using plantCV, a python based image analysis tool developed and maintained by the Data Science Core Facility at DDPSC. The plantCV output from a Bellwether experiment consists of numeric phenotypes commonly broken into two categories, single value traits and multi value traits. Single value traits are phenotypes where one image yields one value, things like plant height or plant area. Multi value traits require multiple numbers to describe a single image and currently are limited to color histograms in various color spaces. Here we will focus only on the hue channel of HSV color, but there are lots of options with your data. This package is being developed to help with common analysis tasks that arise in using plantCV output. If your goal or experiment seems to be unsupported then please consider raising an [issue on github](https://github.com/joshqsumner/pcvr/issues/new/choose) so we can know what directions to take development in for the future.


Installation of `pcvr` from github is possible with the `remotes` or `devtools` packages. 

```
devtools::install_github("joshqsumner/pcvr", build_vignettes = TRUE)
library(pcvr)
```

If you clone that repository and are making edits then you can easily use your local version with:  `devtools::load_all("file/path/to/local/pcvr")`


Functions in `pcvr` use colorblind friendly palettes from the `viridis` package if they specify color/fill scales. Some functions do not specify a color/fill scale and use `ggplot2` defaults. If you use these functions or base work off of them please be mindful of your choices regarding color. 

As a final note before starting into `pcvr`, this vignette is laid out to help guide analyses from Bellwether experiments these functions tend to be generalizable and can be used for other plantCV data as well.


## Read In Data

### `read.pcv`

Bellwether data can be very large so this vignette will use a small bellwether dataset that is already subset to remove most multi-value traits (most color histograms). That data is available on [github](https://github.com/joshqsumner/pcvrTestData/blob/main/smallPhenotyperRun.csv).

The single value traits can be read in and converted to wide format with `read.pcv`, here using `data.table::fread` for speed.

```{r}
sv<-read.pcv("https://media.githubusercontent.com/media/joshqsumner/pcvrTestData/main/smallPhenotyperRun.csv", mode="wide", singleValueOnly = T, reader="fread")

sv$genotype = substr(sv$barcode, 3,5)
sv$genotype = ifelse(sv$genotype == "002", "B73",
                     ifelse(sv$genotype == "003", "W605S",
                            ifelse(sv$genotype == "004", "MM", "Mo17")))
sv$fertilizer = substr(sv$barcode, 8, 8)
sv$fertilizer = ifelse(sv$fertilizer == "A", "100",
                     ifelse(sv$fertilizer == "B", "50", "0"))
```


Sometimes large plantCV output may be too large to be read into memory for R. In that case `read.pcv` has a `filter` argument which will filter rows using awk through linux/unix outside of R. That feature would work as shown below to read only single value traits into memory. This can take a few minutes but allows the entirely workflow to be documented in one R file. 


```{r, eval=F}
example<-read.pcv("prohibitivelyLargeFile.csv",
   filters = list("trait in area, convex_hull_area, solidity, perimeter, width, height, longest_path, convex_hull_vertices, ellipse_major_axis, ellipse_minor_axis, ellipse_angle, ellipse_eccentricity, hue_circular_mean, hue_circular_std, hue_median", "sample in default"))
```

### `read.pcv.bw`

Legacy Bellwether data (plantCV version 3 output) can also be read in with `read.pcv.bw` which is a wrapper around `read.pcv` which attempts to do several common tasks related bellwether data joining internally. This should be considered experimental as it is based on plantCV version 3 and earlier outputs/bellwether experiments can take many formats and this has only been considered with a few datasets. With the older plantCV output the data is already in a wider format so here the default mode is "long". Note that while `read.pcv` works fine with this older data and still has some added benefits the reasons to use it in place of `data.table::fread` or `base::read.csv` are less compelling.

```{r, eval=F}
bw<-read.pcv.bw( file="https://raw.githubusercontent.com/joshqsumner/pcvrTestData/main/pcv3Phenos.csv",metaCol=NULL)

bw<-read.pcv.bw( file="https://raw.githubusercontent.com/joshqsumner/pcvrTestData/main/pcv3Phenos.csv",metaCol="meta", metaForm="vis_view_angle_zoom_horizontal_gain_exposure_v_new_n_rep", joinSnapshot = "id")

bw<-read.pcv.bw( file="https://raw.githubusercontent.com/joshqsumner/pcvrTestData/main/pcv3Phenos.csv",
   snapshotFile="https://raw.githubusercontent.com/joshqsumner/pcvrTestData/main/pcv3Snapshot.csv",
   designFile="https://raw.githubusercontent.com/joshqsumner/pcvrTestData/main/pcv3Design.csv",
   metaCol="meta",metaForm="vis_view_angle_zoom_horizontal_gain_exposure_v_new_n_rep",
   joinSnapshot="id",conversions = list(area=13.2*3.7/46856))
```


### Other metadata

Often we want to convert our timestamp data from the lemnatech into either days after start (DAS), days after planting (DAP), or days after emergence (DAE). By default the `bw.time` function will add columns called DAS, DAP, and DAE to your data. Days after emergence requires using a phenotype and a value to classify emergence. Here an area greater than 10 pixels is considered an emerged plant. In this example with a planting delay of 0 DAP and DAE will be the same, but both are still created for the purpose of the example.

```{r}
sv<-bw.time(sv, plantingDelay = 0, phenotype="area.pixels", cutoff=10, timeCol="timestamp", group=c("barcode", "rotation"), plot=T)
```

If we plot our time data then we might see that there are a few outliers or we might check a summary of our phenotype and see some outliers, either from experiment conditions or from the image analysis workflow. The `bw.outliers` function can be used to remove outliers relative to a phenotype using cook's distance. Here just over 6 percent of data are removed as outliers.

```{r}
sv<-bw.outliers(sv, phenotype="area.pixels", group = c("DAS", "genotype", "fertilizer"), plotgroup = c("barcode", "rotation"))
```

We might also want to check the watering data, which can be read easily from json with `bw.water`.

```{r}
water<-bw.water("https://raw.githubusercontent.com/joshqsumner/pcvrTestData/main/metadata.json")

water$genotype = substr(water$barcode, 3,5)
water$genotype = ifelse(water$genotype == "002", "B73",
                     ifelse(water$genotype == "003", "W605S",
                            ifelse(water$genotype == "004", "MM", "Mo17")))
water$fertilizer = substr(water$barcode, 8, 8)
water$fertilizer = ifelse(water$fertilizer == "A", "100",
                     ifelse(water$fertilizer == "B", "50", "0"))

ggplot(water[water$weight_after!=-1,], aes(x=DAS, y=water_amount, group=barcode,color=interaction(genotype,fertilizer)))+
  geom_line()+
  pcv_theme()+
  guides(color=guide_legend(title="Condition", nrow=3,byrow=T))+
  theme(legend.position="bottom")
```

Finally, we can combine images taken at different angles of the same plant on a given day with `aggregate`, where we will also remove some columns we aren't using. In this case we use the mean of observations, but some people prefer a sum. Either way is fine. Here we also remove the DAE column since it can be different for different angles and we are not concerned about adjusting for emergence here. 

```{r}
phenotypes <- c('area.pixels', 'convex_hull_area.pixels', 'convex_hull_vertices', 'ellipse_angle.degrees', 'ellipse_eccentricity', 'ellipse_major_axis.pixels', 'ellipse_minor_axis.pixels', 'height.pixels', 'hue_circular_mean.degrees', 'hue_circular_std.degrees', 'hue_median.degrees', 'longest_path.pixels', 'perimeter.pixels', 'solidity', 'width.pixels')
phenoForm<-paste0("cbind(", paste0(phenotypes, collapse=", "), ")")
groupForm<-"DAS+DAP+barcode+genotype+fertilizer"
form<-as.formula(paste0(phenoForm, "~", groupForm))
sv<-aggregate(form, data=sv, mean, na.rm=T)
```


## FREM

Now that our data is read in and has undergone some basic quality control we want to know which phenotypes are best explained by our design variables. The `frem` function partitions variance using a fully random effects model (frem). Here we provide our dataframe, the design variables, the phenotypes, and the column representing time. By default `frem` will use the last timepoint, but this is controlled with the `time` argument.

```{r}
frem(sv, des=c("genotype", "fertilizer"),
     phenotypes=c('area.pixels', 'convex_hull_area.pixels', 'convex_hull_vertices', 'ellipse_angle.degrees', 'ellipse_eccentricity', 'ellipse_major_axis.pixels', 'ellipse_minor_axis.pixels', 'height.pixels', 'hue_circular_mean.degrees', 'hue_circular_std.degrees', 'hue_median.degrees', 'longest_path.pixels', 'perimeter.pixels', 'solidity', 'width.pixels'),
     timeCol="DAS", cor=T, returnData=F, combine=F, markSingular=T)
```

Here we look at how much variance in each phenotype was explained over the course of the experiment.

```{r}
frem(sv, des=c("genotype", "fertilizer"),
     phenotypes=c('area.pixels', 'convex_hull_area.pixels', 'convex_hull_vertices', 'ellipse_angle.degrees', 'ellipse_eccentricity', 'ellipse_major_axis.pixels', 'ellipse_minor_axis.pixels', 'height.pixels', 'hue_circular_mean.degrees', 'hue_circular_std.degrees', 'hue_median.degrees', 'longest_path.pixels', 'perimeter.pixels', 'solidity', 'width.pixels'),
     timeCol="DAS", F, F, F, T, "all")
```

This informs our next steps. Hue and size based phenotypes are well explained by our design variables so we might decide to focus more complex analyses on those. In this experiment mini maize is one of the genotypes and the fertilizer treatment has an option with no nitrogen, so this intuitively makes sense and passes an eye check.

## Single Value Traits

Most analysis focus on single value traits, that is phenotypes where one image returns on numeric value such as area or height. These can be compared longitudinally or with respect to individual days. To make this analysis more intuitive we often want to convert our pixel phenotypes into more comprehensible units. Note that we do not recommend using a PCA of the single value traits to look for differences in your treatment groups, these traits are interpretable on their own and are interdependent enough that a PCA is not appropriate.

```{r}
pixels_per_cmsq <- 42.5^2   # pixel per cm^2
sv$area_cm2<-sv$area.pixels / pixels_per_cmsq
sv$height_cm <- sv$height.pixels/42.5
```


### Loess Plots

Trendlines help us decide what next steps make the most sense and give a general impression of which conditions yielded healthier plants.

```{r}
ggplot(sv, aes(x=DAS, y=area_cm2, group=interaction(genotype, fertilizer, lex.order=T),
                                color = interaction(genotype, fertilizer, lex.order=T)))+
  geom_smooth(method="loess", se=T)+
  labs(y=expression("Area"~"(cm"^2~")"),
       color="Genotype\nand Soil")+
  pcv_theme()+theme(axis.text.x.bottom=element_text(angle=0))
```


### Single day comparisons

```{r}
pcvBox(sv[sv$genotype=='MM' & sv$DAS==15, ], x="fertilizer", y="area_cm2", compare="0", showPoints = T)+
  labs(y=expression("Area"~"(cm"^2~")"),
       fill="Soil Fertilizer")
```

### Relative Tolerance

Often bellwether experiments involve comparing stress tolerance between groups. For example in this dataset we might want to know which genotype shows the most resilience to reduced fertilizer. To easily check this we can change out data with `relativeTolerance`. Details on this function can be read with `?pcvr::relativeTolerance`.

```{r}
rt<-relativeTolerance(sv, phenotypes = c("area_cm2", "height_cm"), grouping = c("fertilizer", "genotype", "DAS"), control = "fertilizer", controlGroup="100", method =  c("proportion", "difference", "zscore"), naTo0=T)

pcvBox(rt[rt$DAS==19, ], x="fertilizer", y="area_cm2_proportion", showPoints = T)+
  labs(y=expression("Proportion of Control Area"))+
  facet_wrap(~genotype)+theme(legend.position="none")
```

### Cumulative Phenotypes

Sometimes we might want to use the cumulative difference over time

```{r}
cp<-cumulativePheno(rt, phenotypes = c("area_cm2", "height_cm"), group="barcode", timeCol="DAS")
```


We can check that this worked correctly with trendlines:

```{r}
ggplot(cp, aes(x=DAS, y=area_cm2_csum, color = interaction(genotype, fertilizer), group= barcode))+
  geom_line()+
  theme_minimal()+
  labs(y=expression("Cumulative Sum of Area"~"(cm"^2~")"),
       color="Genotype\nand Soil")
```

And we can use this new phenotype data to analyse data from the entire experiment using just the last day.

```{r}
pcvBox(cp[cp$DAS==19, ], x="fertilizer", y="area_cm2_csum", showPoints = T)+
  labs(y=expression("Cumulative Area"))+
  facet_wrap(~genotype)+theme(legend.position="none")
```



### Longitudinal Modeling

Longitudinal modeling is the most comprehensive way to use the single value traits from a bellwether experiment. Longitudinal modeling can also be complicated compared to single timepoint analyses. Statistical complications including changes in variance, non-linearity, and autocorrelation present potential problems in analyses. To address these we recommend using hierarchical models. `pcvr` attempts to lower the barrier to entry for these models with helper functions for use with `brms`.

#### Growth Model Forms

Based on literature and observed trends there are 6 common growth models that `pcvr` supports. Those are shown here using the `growthSim` function.

```{r}
simdf<-growthSim("logistic", n=20, t=25, params = list("A"=c(200,160), "B"=c(13, 11), "C"=c(3, 3.5)))
l<-ggplot(simdf,aes(time, y, group=interaction(group,id)))+
  geom_line(aes(color=group))+labs(title="Logistic")+theme_minimal()+theme(legend.position="none")

simdf<-growthSim("gompertz", n=20, t=25, params = list("A"=c(200,160), "B"=c(13, 11), "C"=c(0.2, 0.25)))
g<-ggplot(simdf,aes(time, y, group=interaction(group,id)))+
  geom_line(aes(color=group))+labs(title="Gompertz")+theme_minimal()+theme(legend.position="none")

simdf<-growthSim("monomolecular", n=20, t=25, params = list("A"=c(200,160), "B"=c(0.08, 0.1)))
m<-ggplot(simdf,aes(time, y, group=interaction(group,id)))+
  geom_line(aes(color=group))+labs(title="Monomolecular")+theme_minimal()+theme(legend.position="none")

simdf<-growthSim("exponential", n=20, t=25, params = list("A"=c(15, 20), "B"=c(0.095, 0.095)))
e<-ggplot(simdf,aes(time, y, group=interaction(group,id)))+
  geom_line(aes(color=group))+labs(title="Exponential")+theme_minimal()+theme(legend.position="none")

simdf<-growthSim("linear", n=20, t=25, params = list("A"=c(1.1, 0.95)))
ln<-ggplot(simdf,aes(time, y, group=interaction(group,id)))+
  geom_line(aes(color=group))+labs(title="Linear")+theme_minimal()+theme(legend.position="none")

simdf<-growthSim("power law", n=20, t=25, params = list("A"=c(16, 11), "B"=c(0.75, 0.7)))
pl<-ggplot(simdf,aes(time, y, group=interaction(group,id)))+ geom_line(aes(color=group))+
  labs(title="Power Law")+theme_minimal()+theme(legend.position="none")

(l+g+m)/(e+ln+pl)

```

Typically at least one of these models will be a good fit to your bellwether data, with gompertz models being the most broadly useful so far. In this experiment the plants were not germinated before being added to the machine, so we might not see asymptotic size. Still, conceptually we know that these plants will stop growing in the near future so we might use a gompertz model in place of the exponential model that looks most like our loess trendlines.


#### Model setup 

Coding a multilevel bayesian model can be involved. Even with the greatly simplified syntax used by `brms` this can present a barrier to entry for some people who could benefit from using very robust models. To help get around this potential issue for the specific case of measuring growth over time `pcvr` includes several functions to work with `brms`, the first of which is `growthSS`, a self-starter helper function for use with `brms::brm`.

##### submodel options

There are several ways to consider variance over time. By default almost all modeling assumes homoscedasticity, that is constant variance across predictor variables (time here). That assumption is very unrealistic in biological settings since all seeds/seedlings will start from a very low area but will grow differently through the experiment. The `growthSS` function has three options for modeling variance: homo, linear, spline. There are many ways that we could model variance, but these three are implemented for general use. Splines will almost always yield the best fit to your data.

![Different sub model options shown and compared with LOO Information Criterion](subModelComparisons.png)

##### Prior Distributions

An important part of bayesian statistics is setting an appropriate prior. These represent your knowledge about the field and are used along with your collected data to yield results. Priors should generally be weak relative to your data, meaning that if your prior belief is wrong then your experiment can move the posterior distribution away from the prior in a meaningful way.

In `growthSS` priors can be specified as a brmsprior object (in which case it is used as is), a named list (names representing parameters), or a numeric vector, where values will be used to generate lognormal priors with a long right tail. Lognormal priors with long right tails are used because the values for our growth curves are strictly positive and the lognormal distribution is easily interpreted. The tail is a product of the variance, which is assumed to be 0.25 for simplicity and to ensure priors are wide. This means that only a location parameter needs to be provided. If a list is used then each element of the list can be length 1 in which case each group will use the same prior or it can be a vector of the same length as `unique(data$group)` where `group` is your grouping variable from the form argument to `growthSS`. If a vector is used then a warning will be printed to check that the assumed order of groups is correct. The `growthSim` function can be useful in thinking about what a reasonable prior distribution might be, although priors should not be picked by trying to get a great fit by eye to your collected data.

We can check our priors with the `plotPrior` function.

```{r}
priors = list("A" = 130, "B" = 10, "C" = 1.2)
priorPlots<-plotPrior(priors)
priorPlots[[1]]/priorPlots[[2]]/priorPlots[[3]]
```


##### Using `growthSS`

Now we're ready to define the necessary variables in our data and use the `growthSS` function. 

```{r}
sv$group<-interaction(sv$fertilizer, sv$genotype)
```

```{r, eval=F}
ss<-growthSS(model="gompertz", form =  area_cm2~DAS|barcode/group, sigma="spline", df=sv,
             priors = list("A" = 130, "B" = 10, "C" = 1.2))
```

Now we have most of our model components in the `ss` object.

Before trying to fit the model it is generally a good idea to check a plot of the data and make sure you have everything defined correctly.

```{r}
ggplot(sv, aes(x=DAS, y=area_cm2, group=barcode, color=group))+
  geom_line()+theme_minimal()+
  labs(y=expression("Area"~"(cm"^2~")"),
       color="Genotype\nand Soil")
```

This looks okay, there are no strange jumps in the data or glaring problems.


#### Running Models

The `brms` package is not automatically imported by `pcvr`, so before fitting models we would need to load that package.

```{r, eval=F}
library(brms)
```


```{r, eval=F}
fit <- brm(ss$formula, prior = ss$prior, data = ss$df, family = ss$family, 
                           iter = 2000, cores = 4, chains = 4, init = ss$initfun, 
                           control = list(adapt_delta = 0.999, max_treedepth = 20), backend = "cmdstanr")
```

```{r, eval=F}
data(bw_vignette_fit, package = "pcvr")
```


#### Check Model Fit

We can visualize credible intervals from a `brms` model and compare that to our growth trendlines to get an intuitive understanding of how well the model fit. Note that since this vignette does not load brms these are only a picture of the output from `brmPlot` and `brmViolin`. The code is present to run this locally if you have brms installed and choose to.

```{r, eval=F}
brmPlot(fit, form = area_cm2~DAS|barcode/group, df = ss$df)+ 
  labs(y=expression("Area"~"(cm"^2~")"))
```

![](brmPlot1.png)

In this case our model has 12 groups (3 soil conditions in each of 4 genotypes), so we might want to pull out only a few groups to look at. 

```{r, eval=F}
brmPlot(fit, form = area_cm2~DAS|barcode/group, df = ss$df, groups = c("0.B73", "50.B73", "100.B73"))+
  labs(y=expression("Area"~"(cm"^2~")"))
```

![](brmPlot2.png)

#### Test Hypotheses

Now we probably have some ideas about what we want to test in our data. The `brms::hypothesis` function offers incredible flexibility to test all kinds of hypotheses. For some comparisons `pcvr` has a helper function called `brmViolin` to visualize posterior distributions and the posterior probability of some hypotheses associated with them.

```{r, eval=F}
brmViolin(model = fit, params = NULL,
          hyp="num/denom>1.05", compareX = c("0.B73", "50.B73", "100.B73"), againstY = "0.B73",
          group_sep = "[.]", groups_into = c("soil", "genotype"), x="soil", facet="genotype",
          returnData=F)
```

![](brmViolin_vigPlot.png)

This shows that we have a posterior probability greater than 99 percent of an asymptotic size at least 5 percent higher with the 100 type soil when compared against the 0 type soil. Note that this data does not have any plants that reached asymptotic size, so the model uses the incomplete data to estimate where an asymptote would be. In a normal experiment the plants would be more mature but here the asymptote parameter is artificially inflated for the 100 soil treatment group due to their slower growth rate. 

There are a lot of options for how to use this function and even more ways to use `brms::hypothesis`, so this example should not be seen as the only way to compare your models.

#### Comparing Models Over Time

As a final note on `brms`, there is a possiblity of making interesting early stopping rules in a Bayesian framework. Currently this does not have an obvious Bellwether implementation but it should be noted as a benefit of the method. If you have models fit to subsets of your data then the `distPlot` function will show changes in the posterior distribution for some or all of your parameters over time or over another subset variable. Here the growth trend plots are also a legend for the time of each posterior distribution.

```{r, eval=F}
print(load(url("https://raw.githubusercontent.com/joshqsumner/pcvrTestData/main/brmsFits.rdata")))
from3to25<-list(fit_3, fit_5, fit_7, fit_9, fit_11, fit_13, fit_15, fit_17, fit_19, fit_21, fit_23, fit_25)

distributionPlot(fits = from3to25, form = y~time|id/group, params=c("A", "B", "C"), d=simdf)
```


![](distPlot_vigPlot.png)

## Multi Value Traits

Working with multi value traits leads to different statistical challenges than the single value traits. Generally reading the data in as wide format makes for a significantly smaller object in memory terms since there are not lots of duplicated metadata (identifiers for each row when every image has hundreds of rows potentially). Note that for many questions even about color it is not necessary to use the entire color histograms. Make sure that you have a good reason to use the complete color data before going down this particular path for too long. As an example, a very simple comparison of the circular mean of Hue here will show our treatment effect in this data. 

```{r}
ggplot(sv[sv$DAS==18,], aes(x = fertilizer, y=hue_circular_mean.degrees, fill = fertilizer))+
  geom_boxplot(outlier.shape=NA)+
  geom_jitter(width=0.05, size=0.5)+
  scale_fill_manual(values = c(viridis::viridis(3,1,0.1)), breaks = c('0','50','100'))+
  pcv_theme()+
  theme(legend.position = "none")+
  facet_wrap(~genotype, scales="free_x")+
  scale_x_discrete(limits=c('0','50','100'))+
  labs(y="Hue Circular Mean (degrees)", x="Soil and Genotype")

```


```{r}
hue_wide<-read.pcv("https://media.githubusercontent.com/media/joshqsumner/pcvrTestData/main/smallPhenotyperRun.csv", mode="wide", singleValueOnly = T, multiValPattern = "hist", reader="fread")
hue_wide$genotype = substr(hue_wide$barcode, 3,5)
hue_wide$genotype = ifelse(hue_wide$genotype == "002", "B73",
                     ifelse(hue_wide$genotype == "003", "W605S",
                            ifelse(hue_wide$genotype == "004", "MM", "Mo17")))
hue_wide$fertilizer = substr(hue_wide$barcode, 8, 8)
hue_wide$fertilizer = ifelse(hue_wide$fertilizer == "A", "100",
                     ifelse(hue_wide$fertilizer == "B", "50", "0"))
hue_wide<-bw.time(hue_wide,timeCol="timestamp", group="barcode")

phenotypes <- colnames(hue_wide)[19:225]
phenoForm<-paste0("cbind(", paste0(phenotypes, collapse=", "), ")")
groupForm<-"DAS+barcode+genotype+fertilizer"
form<-as.formula(paste0(phenoForm, "~", groupForm))
hue_wide<-aggregate(form, data=hue_wide, mean, na.rm=T)
```


### Joyplots

Joyplots are a common way to look at lots of distributions. Here we check the hue histograms as joyplots using the 18th day and add a new fill for the hue colorspace. Note that this uses the `ggridges` package which relies on some deprecated `ggplot2` features, hopefully this will be addressed soon but in the meantime you may get a warning message using `pcv.joyplot`. Note that the gaussian comparisons use `dlst` from the `extraDistr` package.

```{r, message=F}
p<-pcv.joyplot(hue_wide[hue_wide$DAS==18,], index = "hue_frequencies", group=c("fertilizer", "genotype"), method=NULL,
               trait=NULL,compare=NULL)
p+scale_fill_gradientn(colors = scales::hue_pal(l=65)(360))
```

We can also subset the data and change the group argument to look at very specific comparisons.

```{r, message=F}
p<-pcv.joyplot(hue_wide[hue_wide$fertilizer==0 & hue_wide$DAS %in% c(7,12,18),], index = "hue_frequencies", group=c("DAS", "genotype"),
               trait=NULL,compare=NULL)
p+scale_fill_gradientn(colors = scales::hue_pal(l=65)(360))
```

### Ordination

Ordinations are another common way to look at multi value traits. The `pcadf` function in `pcvr` has several options for how this can work.

```{r}
pcadf(hue_wide, cols = "hue_frequencies", color = c("genotype", "fertilizer"), returnData=F)
```


This is potentially useful but it does not show us the time component well. We can facet by day or add traces easily. Using the trace argument we can view the movement of groups over time, but that can be cluttered.

```{r}
pcadf(hue_wide, cols = "hue_frequencies", color = c("genotype", "fertilizer"),  trace="DAS", returnData=F)
```

When the trace is cluttered it generally helps to either use a facet or to return a list of plots showing a few days at a time in the trace. Both those options are done using the `facet` argument.

```{r}
plots<-pcadf(hue_wide, cols = "hue_frequencies", color = c("genotype", "fertilizer"),  trace="DAS", returnData=F, facet = c(1,2,3,4))

(plots[[1]]+plots[[2]]) / (plots[[3]]+plots[[2]]) + plot_layout(guides = "collect")
```

```{r}
pcadf(hue_wide, cols = "hue_frequencies", color = c("genotype", "fertilizer"),  facet="DAS", returnData=F)
```

### Earth Mover's Distance

Since color data is exported from plantCV as histogram data we can also use Earth Mover's Distance (EMD) to compare images. Conceptually EMD is a distance that quantifies how much work it would take to turn one histogram into another. Here we do pairwise comparisons of all our rows and return a long dataframe of those distances. Note that even running several cores in parallel this can take a lot of time for larger datasets since the number of comparisons quickly can become unwieldy. The output also will require more work to keep analyzing, so make sure this is what you want to be doing before using EMD to compare color histograms. If you are only interested in a change of the mean then this is probably not the best way to use your data.

Here is a fast example of a place where EMD makes a lot of sense. In this simulated data we have five generating distributions. Normal, Log Normal, Bimodal, Trimodal, and Uniform. We could use some gaussian mixtures to characterize the multi-modal histograms but that will get clunky for comparing to the unimodal or uniform distributions. Instead, we can use EMD.



```{r}
set.seed(123)

simFreqs<-function(vec, group){
  s1<-hist(vec, breaks=seq(1,181,1), plot=F)$counts
  s1d<-as.data.frame(cbind(data.frame(group), matrix(s1,nrow=1)))
  colnames(s1d)<-c('group', paste0("sim_",1:180))
  s1d
}

sim_df<-rbind(do.call(rbind, lapply(1:10, function(i){ simFreqs(rnorm(200, 50, 10), group="normal") })),
      do.call(rbind, lapply(1:10, function(i){ simFreqs(rlnorm(200, log(30),0.25), group="lognormal") })),
      do.call(rbind, lapply(1:10, function(i){ simFreqs( c(rlnorm(125, log(15),0.25), rnorm(75, 75,5) ), group="bimodal") })),
      do.call(rbind, lapply(1:10, function(i){ simFreqs( c(rlnorm(100, log(15),0.25), rnorm(50, 50,5), rnorm(50, 90,5) ), group="trimodal") })),
      do.call(rbind, lapply(1:10, function(i){ simFreqs( runif(200,1,180), group="uniform") }))
      )

#* plot the different groups, run emd, show failings for A vs B

sim_df_long<-as.data.frame(data.table::melt(data.table::as.data.table(sim_df), id.vars = "group"))
sim_df_long$bin<-as.numeric(sub("sim_", "", sim_df_long$variable))

ggplot(sim_df_long, aes(x=bin, y=value, fill=group), alpha=0.25)+
  geom_col(position="identity", show.legend = F)+
  pcv_theme()+
  facet_wrap(~group)
```



Our plots show very different distributions, so we get EMD between our images and see that we do have some trends shown in the resulting heatmap.



```{r}
sim_emd<-pcv.emd(df = sim_df, cols="sim_", reorder=c("group"),
        mat =F, plot=T, parallel = 1, raiseError=T)
sim_emd$plot
```



Now we can filter edge strength during our network building step for EMD > 0.5, and plot our network.



```{r}
emd = sim_emd$data; meta = NULL; dissim=T; distCol="emd"; filter = 0.5; direction="greater"
n<-pcv.net(sim_emd$data, filter = 0.5)
net.plot(n, fill="group")
```


The distributions separate very well from each other, but we don't actually see our uniform distribution here. That is because the uniform distribution does not have self-similar replicates and is also not very similar to the other distributions. If we change our filtering then we can even see which generating distributions are most similar to each other. Here we pass 0.5 as a string, which tells `pcv.net` to use the top 50 percent of EMD values instead of EMD values > 0.5.



```{r}
n<-pcv.net(sim_emd$data, filter = '0.5')
net.plot(n, fill="group")
```


Just as we'd expect, our uniform distribution shows up now and is the most different. Now changing the edgeFilter in the `net.plot` function would let us fine tune this plot more to show finer distinctions between our other four generating distributions.


Here is an example using our hue data, note that these examples will just loading the rdata that was generated by the code below, which took around 3 minutes to run. 


```{r, eval=F}
EMD<-pcv.emd(df = hue_wide[hue_wide$DAS %in% c(5,12,19),], cols="hue_frequencies", reorder=c("fertilizer", "genotype", "DAS"),
             mat =F, plot=T, parallel = 12, raiseError=T) 
save(EMD, file="/home/jsumner/Desktop/stargate/fahlgren_lab/pcvr/data/bw_vignette_small_emd.rda")
```

```{r, eval=F, include=F}
data(bw_vignette_small_emd, package = "pcvr")
# print(load("/home/jsumner/Desktop/stargate/fahlgren_lab/pcvr/data/bw_vignette_small_emd.rda"))
```

```{r, eval=F, include=F}
head(EMD$data)
EMD$plot
```

EMD can get very heavy with large datasets. For a recent lemnatech dataset using only the images from every 5th day there were $6332^2$ = 40,094,224 pairwise EMD values. In long format that's a 40 million row dataframe, which is unwieldy. To get around this problem we might decide to use the hue circular mean as a single value trait or to aggregate some of our data with the `mv_ag` function.

Starting with our complete hue data we have `r nrow(hue_wide)` image histograms. The `mv_ag` function will take a group argument and randomly pick members of that group to have their histograms combined. Note that histograms are scaled to sum to 1 before they are sampled. Here we return one example with 2 histograms kept per group and one example where groups are summarized into 1 histogram. If there are equal or fewer images as the `n_per_group` argument then no aggregation is done for that group but data are rescaled.

```{r}
hue_ag1<-mv_ag(df=hue_wide, group = c("DAS", "genotype", "fertilizer"),  n_per_group=2, keep=c("area.pixels", "height.pixels"))
dim(hue_ag1)

hue_ag2<-mv_ag(hue_wide, group = c("DAS", "genotype", "fertilizer"),  n_per_group=1, keep=c("area.pixels", "height.pixels"))
dim(hue_ag2)
```


### Network Analysis

As it stands our EMD data is potentially difficult to use for those unfamiliar with distance matrix based analysis. Here we represent our distances as a network to help use the results.

```{r, eval=F}
net<-pcv.net(EMD$data, meta = c("fertilizer", "genotype", "DAS"), filter=0.5)
```

```{r, eval=F}
net.plot(net, fill = "DAS", shape = "fertilizer", size=2)
```

This is a much more complicated network! Parsing biological meaning out of this would require more work than the first example with 5 very different distributions. In general if you pick to use EMD with or without networks start small and consider what each step should mean conceptually for your experiment.

## Conclusion

This vignette will be periodically updated as `pcvr` changes. Once again if your goal or experiment has some set of questions or needs that are not met so far please consider making an [issue on github](https://github.com/joshqsumner/pcvr/issues/new/choose) to help the Data Science Core continue to work on this project in ways that will help the plantCV community.












