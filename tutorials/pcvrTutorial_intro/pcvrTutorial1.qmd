---
title: "Intro to `pcvr`"
subtitle: "DDPSC Data Science Core, July 2023"
author: Josh Sumner
format:
  revealjs:
    standalone: true
    self-contained: true
    theme: [moon, ../quartoSupport/solarizedDark.scss]
    scrollable: true
    title-slide-attributes: 
      data-background-image: ../quartoSupport/datascience.png
      data-background-size: 15%
      data-background-position: 2% 2%
editor: visual
---

## Outline {.smaller}

-   `pcvr` Goals
-   Load Package
-   Read in Data
-   Join metadata
-   Remove outliers
-   Single Value Trait Analysis
-   Longitudinal Analysis
-   Resources

## `pcvr` Goals

Currently `pcvr` aims to:

-   Make common tasks easier and consistent
-   Make select Bayesian statistics easier

There is room for goals to evolve based on feedback and scientific needs.

## Load package

Pre-work was to install R, Rstudio, and `pcvr`.

```{r}
#| echo: false
#| eval: true
if(!"pcvr" %in% installed.packages()){
  if(!"devtools" %in% installed.packages()){
   install.packages("devtools") 
  }
  devtools::install_github("danforthcenter/pcvr")
}
```

```{r}
#| echo: true
#library(pcvr)
devtools::load_all("~/Desktop/stargate/fahlgren_lab/pcvr")
library(data.table) # for fread
library(ggplot2) # to change pcvr output plots some
```

## Read in Data

Some plantCV output comes as a very long csv file.

Handling large experiments/those with lots of images can be difficult if the output is multiple Gb.

. . .

The `read.pcv` function helps simplify common tasks related to getting your data ready to use in R.

## Read in Data (2)

`read.pcv` takes several arguments but only the filepath argument is required, most other arguments are for special cases.

```{r}
#| echo: true
args("read.pcv")
```

## Read in Data (3) {.smaller}

To see details about arguments use `?read.pcv`. Three main features are:

-   `mode` Will automatically handle widening/lengthening data on read in.
-   `reader` allows for use of `data.table::fread`, `vroom::vroom`, or other functions to improve speed with large data. This defaults to NULL where `utils::read.csv` is used.
-   `filters` allows Unix based systems to subset local large data without reading it into memory.

## Read in Data (4)

There are datasets online for use with `pcvr` examples, here we read one in as a wide dataframe.

```{r}
#| echo: true
sv<-read.pcv(paste0("https://raw.githubusercontent.com/joshqsumner/",
             "pcvrTestData/main/pcv4-single-value-traits.csv"),
             reader="fread")
dim(sv)
```

. . .

```{r}
#| echo: true
colnames(sv)[19:45]
```

## [Read in Data (exercise)]{style="color:#FFD900;"}

Read in the same example dataset from the previous slides as both wide and long formats. Compare the dimensions of the dataframes and their size in memory.

## [Read in Data (solution)]{style="color:#FF007B;"}

```{r}
#| echo: true
#| eval: false
svw<-read.pcv(paste0("https://raw.githubusercontent.com/joshqsumner/",
             "pcvrTestData/main/pcv4-single-value-traits.csv"),
          mode="wide", reader="fread")
dim(svw) # [1] 2824   45
object.size(svw) # 1436472 bytes

svl<-read.pcv(paste0("https://raw.githubusercontent.com/joshqsumner/",
             "pcvrTestData/main/pcv4-single-value-traits.csv"),
          mode="long", reader="fread")
dim(svl) # [1] 76248    21
object.size(svl) # 11183544 bytes
10070624 / 1304024 # 7.7 times larger in long format
```

Why does the long data take up so much more space?

. . .

```{r}
#| eval: false
#| echo: true
range(which(colnames(svl) %in% colnames(svw))) # 1:18
```

## Join Metadata

We can either parse the barcodes for design information or join a key file using common barcodes.

```{r}
#| echo: true
key<-read.csv(paste0(
  "https://raw.githubusercontent.com/joshqsumner/",
  "pcvrTestData/main/smallPhenotyperRun_key.csv"))
sv<-merge(sv, key, by="barcode")
```

. . .

Data Joining tends to be straightforward enough, but other metadata tasks can be more involved.

## Handling time with `bw.time`

The `bw.time` function parses timestamps into (optionally) several kinds of time labels.

Here we show a few examples before using the default behavior to make DAS, DAP, and DAE (days after start, planting, and emergence).

## Handling time with `bw.time` (2)

`bw.time` takes an optional `mode` argument, which can control how many types of time are output. Here we make Days after Start (DAS).

```{r}
#| echo: true
#| output-location: slide
ex<-bw.time(sv, mode="DAS", phenotype="area_pixels",
           group=c("barcode", "rotation"), plot=T)
```

## Handling time with `bw.time` (3)

We can also make Days after Emergence with `mode="DAE"`, using a cutoff for emergence on a phenotype.

```{r}
#| echo: true
#| output-location: slide
ex<-bw.time(sv, mode="DAE", phenotype="area_pixels",
               cutoff=10, group=c("barcode", "rotation"), plot=T)
```

## Handling time with `bw.time` (4)

```{r}
#| echo: true
svt <- bw.time(sv, plantingDelay = 0, phenotype="area_pixels",
               cutoff=10, group=c("barcode", "rotation"), plot=F)
```

. . .

```{r}
svt[1:8, c("timestamp", "DAS", "DAP", "DAE", "area_pixels")]
```

## Remove Outliers

Before removing outliers we aggregate across photos on the same day of the same plant.

```{r}
#| code-line-numbers: "1,3"
#| echo: true
phenotypes <- colnames(svt)[19:45]
phenoForm<-paste0("cbind(", paste0(phenotypes, collapse=", "), ")")
groupForm<-"DAS+barcode+genotype+fertilizer"
form<-as.formula(paste0(phenoForm, "~", groupForm))
sv_ag_withOutliers<-aggregate(form, data=svt, mean, na.rm=T)
dim(sv_ag_withOutliers)
```

. . .

Now we can remove outliers as the last step before getting into analysis.

## Removing Outliers with `bw.outliers`

The `bw.outliers` function uses Cook's Distance to detect outliers.

. . .

Cook's Distance is model (glm) based, so we detect outliers on a per-phenotype (outcome) basis per some set of groups (design variables).

. . .

```{r}
#| echo: true
#| output-location: slide
sv_ag<-bw.outliers(df = sv_ag_withOutliers,
                   phenotype="area_pixels",
                   group = c("DAS", "genotype", "fertilizer"),
                   plotgroup = c("barcode"))
```

## Single Value Trait Analysis

Some of our most common goals involve comparing single value traits at a given time.

Even these simple type of questions can take multiple forms, but before getting into those we'll rescale our area phenotype to $\text{cm}^2$.

```{r}
#| echo: true
pxTocm2 <- 42.5^2 # 51^2 / 1.2^2 : px per cm^2 / color chip size
sv_ag$area_cm2<-sv_ag$area_pixels / pxTocm2
```

## What are we asking?

-   Are the means (or distributions) of X groups different?
-   Is the difference biologically meaningful?

## T tests with pcvBox

`pcvBox` is a ggplot and ggpubr wrapper with easy pairwise difference of means tests in mind.

```{r}
#| echo: true
day19<-sv_ag[sv_ag$genotype=='W605S' & sv_ag$DAS==19, ]

box <- pcvBox(day19, x="fertilizer", y="area_cm2",
               compare="50", showPoints = T)
```

## T tests with pcvBox (2)

```{r}
#| echo: true
#| output-location: slide
(box<-box+
  labs(y=expression("Area"~"(cm"^2~")"),
       fill="Soil Fertilizer")+
  scale_x_discrete(limits = c("0", "50", "100")) )
```

## [pcvBox (exercise)]{style="color:#FFD900;"}

Compare all genotypes in fully fertilized soil (100) areas against B73 on day 19 using a non-parametric test (wilcox)

## [pcvBox (solution)]{style="color:#FF007B;"}

```{r}
#| echo: true
ex<-sv_ag[sv_ag$fertilizer=='100' & sv_ag$DAS==19, ]
pcvBox(ex, x="genotype", y="area_cm2",
               compare="B73", method = "wilcox", showPoints = T)
```

## Interpreting P-values

```{r}
print(box)
```

Based on an $\alpha$ of 0.05 we would say that B73 and Mo17 have statistically significant differences in size 19 days after planting.

. . .

But correct interpretation does not tell us the probability that one genotype is larger than the other.

## Interpreting P-values (2)

![xkcd P-values](https://imgs.xkcd.com/comics/p_values.png)

## Interpreting P-values (3)

Correct interpretation: If these two genotypes have the same area at the population level then there would be a $\le5\%$ chance of estimating the observed difference.

We can ask more specific questions using different tools.

## Posterior Probability

In Bayesian statistics posterior probability is used instead of p-values.

The interpretation is much simpler, posterior probability is the probability that some hypothesis is true given the observed data and prior evidence.

## Bayesian vs Frequentist {.smaller}

In a Bayesian context we flip "random" and "fixed" elements.

|             | Fixed                                   | Random                                  | Interpretation                                                                                                                                                             |
|------------------|------------------|------------------|------------------|
| Frequentist | [True Effect]{style="color:goldenrod;"} | [Data]{style="color:purple;"}           | If the [True Effect is 0]{style="color:goldenrod;"} then there is an $\alpha\cdot100$% chance of [estimating an effect]{style="color:purple;"} of this size or more.       |
| Bayesian    | [Data]{style="color:purple;"}           | [True Effect]{style="color:goldenrod;"} | Given the [estimated effect from our data]{style="color:purple;"} there is a P probability of the [True Effect]{style="color:goldenrod;"} being a difference of at least X |

## Bayesian vs Frequentist (2)

You should pick statistical methods and your design models based on your research questions.

Be aware of the advantages and limitations that come with your phrasing of a question and chosen method.

One of the goals in `pcvr` is to make it easier to answer a broader range of questions.

## `conjugate` function

`conjugate` uses conjugate priors and moments from given distributions to make computationally light Bayesian comparisons of single or multi value traits.

Several methods are available for different distributions.

## `conjugate` function (2) {.smaller}

-   t: Difference of means for gaussian data.
-   gaussian: Difference of distributions for gaussian data.
-   beta: Difference of distributions for data bounded \[0,1\]
-   lognormal: Difference of distributions for log normal data (right skew)
-   poisson: Difference of distributions for count data
-   negative binomial: Alternative difference of distributions for count data (given "r")
-   dirichlet: Difference of categorical distributions (2 versions)

## `conjugate` function (3)

```{r}
#| echo: false
#| output-location: slide
library(extraDistr)
methods<-c("lognormal","t","gaussian","beta","poisson","negbin")
p1 <- ggplot(data.frame(x=seq(0, 250, 0.1),
                        fill=factor("lognormal", levels = methods)),
             aes(x=x, fill=fill))+
  stat_function(geom="polygon", fun = dlnorm, args = list(meanlog=log(70), sdlog=log(1.5)))+
  scale_fill_manual(values=viridis::viridis(6,1, 0.1, 0.9), drop = F)+
  pcv_theme()+
  labs(title="lognormal")

p2 <- ggplot(data.frame(x=seq(0, 20, 0.01), 
                        fill=factor("t", levels =methods)),
             aes(x=x, fill=fill))+
  stat_function(geom="polygon", fun = dlst, args = list(df = 3, mu = 10, sigma = 2))+
  scale_fill_manual(values=viridis::viridis(6,1, 0.1, 0.9), drop = F)+
  pcv_theme()+
  labs(title="T")

p3 <- ggplot(data.frame(x=seq(0, 20, 0.001),
                        fill=factor("gaussian", levels = methods)),
             aes(x=x, fill=fill))+
  stat_function(geom="polygon", fun = dnorm, args = list(mean = 10, sd = 2))+
  scale_fill_manual(values=viridis::viridis(6,1, 0.1, 0.9), drop = F)+
  pcv_theme()+
  labs(title="gaussian")

p4 <- ggplot(data.frame(x=seq(0, 1, 0.0001),
                        fill=factor("beta", levels = methods)),
             aes(x=x, fill=fill))+
  stat_function(geom="polygon", fun = dbeta, args = list(shape1=10, shape2=5))+
  scale_fill_manual(values=viridis::viridis(6,1, 0.1, 0.9), drop = F)+
  pcv_theme()+
  labs(title="beta")

set.seed(123)
pois<-hist(rpois(100000, 12), breaks=seq(0,30,1), plot=F)
p5 <- ggplot(data.frame(x=pois$breaks[-31], y=pois$counts, 
                        fill=factor("poisson",levels = methods)),
             aes(x=x, y=y, fill=fill))+
  geom_col()+
  scale_fill_manual(values=viridis::viridis(6,1, 0.1, 0.9), drop = F)+
  pcv_theme()+
  labs(title="poisson")

set.seed(123)
nb<-hist(rnbinom(100000, 5, 0.2), breaks=seq(0,100,1), plot=F)
p6 <- ggplot(data.frame(x=nb$breaks[-101], y=nb$counts,
                        fill=factor("negbin", levels = methods)),
             aes(x=x, y=y, fill=fill))+
  geom_col()+
  scale_fill_manual(values=viridis::viridis(6,1, 0.1, 0.9), drop = F)+
  pcv_theme()+
  labs(title="negbin")
library(patchwork)
(p1+p2+p3)/(p4+p5+p6)+plot_layout(guides="collect")
```

## Bayesian comparison with `conjugate`

Here is an example of using the `conjugate` function from `pcvr` to do a Bayesian analog to a T test and a region of practical equivalence (*ROPE*) comparison.

```{r}
#| eval: false
#| echo: true
s1 <- sv_ag[sv_ag$fertilizer=='100' & sv_ag$DAS==19 & sv_ag$genotype=="W605S", "area_cm2"]
s2 <- sv_ag[sv_ag$fertilizer=='50' & sv_ag$DAS==19 & sv_ag$genotype=="W605S", "area_cm2"]
conj<-conjugate(s1, s2, method="t", plot=T,
                priors = list( mu=c(20,20),n=c(1,1),s2=c(20,20) ),
                rope_range=c(-10,10),
                support = seq(0,125,0.1))
```

## Bayesian comparison with `conjugate`

```{r}
#| echo: false
s1 <- sv_ag[sv_ag$fertilizer=='100' & sv_ag$DAS==19 & sv_ag$genotype=="W605S", "area_cm2"]
s2 <- sv_ag[sv_ag$fertilizer=='50' & sv_ag$DAS==19 & sv_ag$genotype=="W605S", "area_cm2"]
conj<-conjugate(s1, s2, method="t", plot=T,
                priors = list( mu=c(20,20),n=c(1,1),s2=c(20,20) ),
                rope_range=c(-10,10), support = seq(0,125,0.1))
```

## Bayesian comparison with `conjugate` (2)

`conjugate` returns a list with several useful components.

```{r}
#| echo: true
lapply(conj, class)#[1:length(conj)]
```

## Bayesian comparison with `conjugate` (2)

The summary item contains HDE and HDI estimates for samples and their difference (if two samples are provided).

```{r}
#| echo: true
conj$summary
```

The posterior item is an updated set of priors. It can be recycled as more data is collected in Bayesian updating, but the practical speed difference is trivial.

The remaining items are the plot and data that was used in making the plot.

## [conjugate (exercise)]{style="color:#FFD900;"}

Convert height from px to cm and report the probability that the difference in height between B73 in 50% fertilizer and W605x in 0% fertilizer is larger than $|10\text{cm}|$.

## [conjugate (solution)]{style="color:#FF007B;"}

```{r}
#| echo: true
s1 <- sv_ag[sv_ag$fertilizer=='50' & sv_ag$DAS==19 &
              sv_ag$genotype=="B73", "height_pixels"]/42.5
s2 <- sv_ag[sv_ag$fertilizer=='0' & sv_ag$DAS==19 &
              sv_ag$genotype=="W605S", "height_pixels"]/42.5

conj<-conjugate(s1,s2, method="gaussian", plot=T,
                priors = list( mu=c(20,20),n=c(1,1),s2=c(20,20) ),
                support = seq(0,60,0.1), rope_range = c(-10,10))
# 1-conj$summary$rope_prob: 0.318
```

## Longitudinal Analysis

One of the core benefits of image based high throughput phenotyping is that you can quickly collect non-destructive samples.

Keeping the plants alive allows for longitudinal data on individuals.

## Longitudinal Analysis Challenges

These benefits add complexity to our data.

Longitudinal data is:

-   Autocorrelated
-   Often non-linear
-   Heteroskedastic

## Modeling longitudinal data well

To account for autocorrelation, non-linearity, and heteroskedasticity (changes in variance over time) we need to use non-linear mixed effects models. While this can be done with `nlme` these models in `pcvr` use the greater flexibility of `brms` and gain the benefits of Bayesian methods.

## Data preparation

For this example we subset the data, but we could also model the entire dataset by making a new `group` column that has fertilizer and genotype information.

```{r}
#| echo: true
sub<-sv_ag[sv_ag$genotype=="W605S",]
sub$fertilizer=as.character(sub$fertilizer)
```

To set up our model it generally helps to plot the data and make sure we can use an appropriate model and can identify our groups and individuals.

```{r}
#| echo: true
#| output-location: slide
ggplot(sub, aes(x= DAS, y=area_cm2, group=barcode, color=fertilizer))+
  geom_line()+
  theme_minimal()
```

## Using `growthSS`

The `growthSS` function serves as a "self starter" for 6 common growth models (logistic, gompertz, monomolecular, linear, exponential, and power law).

```{r}
#| echo: true
#| eval: false
ss<-growthSS(model="gompertz",
             form =  area_cm2~DAS|barcode/fertilizer,
             sigma="spline",
             df=sub,
             priors = list("A" = 130, "B" = 15, "C" = 3))
```

See `?growthSS` and `?growthSim` for details.

## Model fitting

If you are using one of the 6 common growth models supported by `growthSS` then you can use `fitGrowth` to fit the model using Stan.

```{r}
#| eval: false
#| echo: true
fit_test <- fitGrowth(ss, iter = 1000,
                      cores = 2, chains = 2, backend = "cmdstanr",
               control = list(adapt_delta = 0.999, max_treedepth = 20))
```

```{r}
#| eval: false
#| echo: true
summary(fit_test)
```

## Understanding your model

`pcvr` has two plotting functions for `brmsfit` models:

-   `brmPlot` will show credible intervals along the model fit

-   `brmViolin` will show posterior distributions and probability of given hypotheses.

## Resources

For a more complete workflow using a small bellwether dataset see the `bellwether` vignette.

```{r}
#| eval: false
#| echo: true
browseVignettes("pcvr") # requires you to have build vignettes on installation
# see installation document
```

The `help-datascience` slack channel and `pcvr` [github repository](https://github.com/danforthcenter/pcvr) are good places to ask questions.
