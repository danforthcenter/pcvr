---
title: "Intro to `pcvr`"
subtitle: "DDPSC Data Science Core, July 2023"
author: Josh Sumner
format:
  revealjs:
    standalone: true
    self-contained: true
    theme: [moon, ../quartoSupport/solarizedDark.scss]
    scrollable: true
    title-slide-attributes: 
      data-background-image: ../quartoSupport/datascience.png
      data-background-size: 15%
      data-background-position: 2% 2%
editor: visual
---

## Outline {.smaller}

-   `pcvr` Goals
-   Load Package
-   Read in Data
-   Join metadata
-   Remove outliers
-   Single Value Trait Analysis
-   Longitudinal Analysis
-   Resources

## `pcvr` Goals

Currently `pcvr` aims to:

-   Make common tasks easier and consistent
-   Make select Bayesian statistics easier

There is room for goals to evolve based on feedback and scientific needs.

## Load package

Pre-work was to install R, Rstudio, and `pcvr`.

```{r}
#| echo: false
#| eval: true
if(!"pcvr" %in% installed.packages()){
  if(!"devtools" %in% installed.packages()){
   install.packages("devtools") 
  }
  devtools::install_github("danforthcenter/pcvr")
}
```

```{r}
#| echo: true
library(pcvr)
library(data.table) # for fread
library(ggplot2) # to change pcvr output plots some
library(patchwork) # to combine ggplots
```

## Read in Data

Some plantCV output comes as a very long csv file.

Handling large experiments/those with lots of images can be difficult if the output is multiple Gb.

. . .

The `read.pcv` function helps simplify common tasks related to getting your data ready to use in R.

## Read in Data (2)

`read.pcv` takes several arguments but only the filepath argument is required, most other arguments are for special cases.

```{r}
#| echo: true
args("read.pcv")
```

## Read in Data (3) {.smaller}

To see details about arguments use `?read.pcv`. Three main features are:

-   `mode` Will automatically handle widening/lengthening data on read in.
-   `reader` allows for use of `data.table::fread`, `vroom::vroom`, or other functions to improve speed with large data. This defaults to NULL where `utils::read.csv` is used.
-   `filters` allows Unix based systems to subset local large data without reading it into memory.

## Read in Data (4)

There are datasets online for use with `pcvr` examples, here we read one in as a wide dataframe.

```{r}
#| echo: true
sv<-read.pcv(paste0("https://raw.githubusercontent.com/joshqsumner/",
             "pcvrTestData/main/pcv4-single-value-traits.csv"),
             reader="fread")
dim(sv)
```

. . .

```{r}
#| echo: true
colnames(sv)[19:45]
```

## [Read in Data (exercise)]{style="color:#FFD900;"}

Read in the same example dataset from the previous slides as both wide and long formats. Compare the dimensions of the dataframes and their size in memory.

## [Read in Data (solution)]{style="color:#FF007B;"}

```{r}
#| echo: true
#| eval: false
svw<-read.pcv(paste0("https://raw.githubusercontent.com/joshqsumner/",
             "pcvrTestData/main/pcv4-single-value-traits.csv"),
          mode="wide", reader="fread")
dim(svw) # [1] 2824   45
object.size(svw) # 1304192 bytes

svl<-read.pcv(paste0("https://raw.githubusercontent.com/joshqsumner/",
             "pcvrTestData/main/pcv4-single-value-traits.csv"),
          mode="long", reader="fread")
dim(svl) # [1] 77058    20
object.size(svl) # 10070624 bytes
10070624 / 1304024 # 7.7 times larger in long format
```

Why does the long data take up so much more space?

. . .

```{r}
#| eval: false
#| echo: true
range(which(colnames(svl) %in% colnames(svw))) # 1:18
```

## Join Metadata

We can either parse the barcodes for design information or join a key file using common barcodes.

```{r}
#| echo: true
key<-read.csv(paste0(
  "https://raw.githubusercontent.com/joshqsumner/",
  "pcvrTestData/main/smallPhenotyperRun_key.csv"))
sv<-merge(sv, key, by="barcode")
```

. . .

Data joining tends to be straightforward enough, but other metadata tasks can be more involved.

## Handling time with `bw.time`

The `bw.time` function parses timestamps into (optionally) several kinds of time labels.

Here we show a few examples before using the default behavior to make DAS, DAP, and DAE (days after start, planting, and emergence).

## Handling time with `bw.time` (2)

`bw.time` takes an optional `mode` argument, which can control how many types of time are output. Here we make Days after Start (DAS).

```{r}
#| echo: true
#| output-location: slide
ex<-bw.time(sv, mode="DAS", phenotype="area_pixels",
           group=c("barcode", "rotation"), plot=T)
```

## Handling time with `bw.time` (3)

We can also make Days after Emergence with `mode="DAE"`, using a cutoff for emergence on a phenotype.

```{r}
#| echo: true
#| output-location: slide
ex<-bw.time(sv, mode="DAE", phenotype="area_pixels",
               cutoff=10, group=c("barcode", "rotation"), plot=T)
```

## Handling time with `bw.time` (4)

```{r}
#| echo: true
svt <- bw.time(sv, plantingDelay = 0, phenotype="area_pixels",
               cutoff=10, group=c("barcode", "rotation"), plot=F)
```

. . .

```{r}
svt[1:8, c("timestamp", "DAS", "DAP", "DAE", "area_pixels")]
```

## Remove Outliers

Before removing outliers we aggregate across photos on the same day of the same plant.

```{r}
#| code-line-numbers: "1,3"
#| echo: true
phenotypes <- colnames(svt)[19:45]
phenoForm<-paste0("cbind(", paste0(phenotypes, collapse=", "), ")")
groupForm<-"DAS+barcode+genotype+fertilizer"
form<-as.formula(paste0(phenoForm, "~", groupForm))
sv_ag_withOutliers<-aggregate(form, data=svt, mean, na.rm=T)
dim(sv_ag_withOutliers)
```

. . .

Now we can remove outliers as the last step before getting into analysis.

## Removing Outliers with `bw.outliers`

The `bw.outliers` function uses Cook's Distance to detect outliers.

. . .

Cook's Distance is model (glm) based, so we detect outliers on a per-phenotype (outcome) basis per some set of groups (design variables).

. . .

```{r}
#| echo: true
#| output-location: slide
sv_ag<-bw.outliers(df = sv_ag_withOutliers,
                   phenotype="area_pixels",
                   group = c("DAS", "genotype", "fertilizer"),
                   plotgroup = c("barcode"))
```

## Single Value Trait Analysis

Some of our most common goals involve comparing single value traits at a given time.

Even these simple type of questions can take multiple forms, but before getting into those we'll rescale our area phenotype to $\text{cm}^2$.

```{r}
#| echo: true
pxTocm2 <- 42.5^2 # 51^2 / 1.2^2 : px per cm^2 / color chip size
sv_ag$area_cm2<-sv_ag$area_pixels / pxTocm2
```

## What are we asking?

-   Are the means (or distributions) of X groups different?
-   Is the difference biologically meaningful?

## T tests with pcvBox

`pcvBox` is a ggplot and ggpubr wrapper with easy pairwise difference of means tests in mind.

```{r}
#| echo: true
day19<-sv_ag[sv_ag$genotype=='W605S' & sv_ag$DAS==19, ]

box <- pcvBox(day19, x="fertilizer", y="area_cm2",
               compare="50", showPoints = T)
```

## T tests with pcvBox (2)

```{r}
#| echo: true
#| output-location: slide
(box<-box+
  labs(y=expression("Area"~"(cm"^2~")"),
       fill="Soil Fertilizer")+
  scale_x_discrete(limits = c("0", "50", "100")) )
```

## [pcvBox (exercise)]{style="color:#FFD900;"}

Compare all genotypes in fully fertilized soil (100) areas against B73 on day 19 using a non-parametric test (wilcox)

## [pcvBox (solution)]{style="color:#FF007B;"}

```{r}
#| echo: true
ex<-sv_ag[sv_ag$fertilizer=='100' & sv_ag$DAS==19, ]
pcvBox(ex, x="genotype", y="area_cm2",
               compare="B73", method = "wilcox", showPoints = T)
```

## Interpreting P-values

```{r}
print(box)
```

Based on an $\alpha$ of 0.05 we would say that B73 and Mo17 have statistically significant differences in size 19 days after planting.

. . .

But correct interpretation does not tell us the probability that one genotype is larger than the other.

## Interpreting P-values (2)

![xkcd P-values](https://imgs.xkcd.com/comics/p_values.png)

## Interpreting P-values (3)

Correct interpretation: If these two genotypes have the same area at the population level then there would be a $\le5\%$ chance of estimating the observed difference.

We can ask more specific questions using different tools.

## Posterior Probability

In Bayesian statistics posterior probability is used instead of p-values.

The interpretation is much simpler, posterior probability is the probability that some hypothesis is true given the observed data and prior evidence.

## Bayesian vs Frequentist {.smaller}

In a Bayesian context we flip "random" and "fixed" elements.

|             | Fixed                                   | Random                                  | Interpretation                                                                                                                                                             |
|-------------|-----------------------------------------|-----------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Frequentist | [True Effect]{style="color:goldenrod;"} | [Data]{style="color:purple;"}           | If the [True Effect is 0]{style="color:goldenrod;"} then there is an $\alpha\cdot100$% chance of [estimating an effect]{style="color:purple;"} of this size or more.       |
| Bayesian    | [Data]{style="color:purple;"}           | [True Effect]{style="color:goldenrod;"} | Given the [estimated effect from our data]{style="color:purple;"} there is a P probability of the [True Effect]{style="color:goldenrod;"} being a difference of at least X |

## Bayesian vs Frequentist (2)

You should pick statistical methods and your design models based on your research questions.

Be aware of the advantages and limitations that come with your phrasing of a question and chosen method.

One of the goals in `pcvr` is to make it easier to answer a broader range of questions.

## Conjugacy

Historically Bayesian methods have been difficult to implement because of computational complexitity.

. . .

Using "conjugate" prior distributions makes simple questions very quick to answer.

## Conjugacy Example

```{r}
#| echo: false
#| eval: true
bdf<-data.frame(
  value = c(rbeta(5000,2,2), rbeta(5000, 20, 20), rbeta(5000, 40, 10), rbeta(5000,20, 80), rbeta(5000,2,8), rbeta(5000,100,10)),
  Beta = factor(c(rep(c("B(2,2)", "B(20,20)", "B(40,10)", "B(20,80)", "B(2,8)", "B(100,10)"), each = 5000)),
                levels=c("B(2,2)", "B(20,20)", "B(40,10)", "B(20,80)", "B(2,8)", "B(100,10)"))
)

pal<-viridis::viridis(6) # should be imported by pcvr and ggplot2

beta_dist_plot<-ggplot(bdf, aes(x = value, fill=Beta))+
  geom_density(alpha=0, color=NA)+scale_fill_viridis_d()+
  guides(fill=guide_legend(override.aes = list(alpha=1)))+
  labs(x="Percentage", y = "Density")+
  stat_function(fun = dbeta, geom = "polygon", colour = pal[1],
                fill = pal[1], alpha = 0.75, args=list(shape1=2, shape2=2), show.legend=F)+
  stat_function(fun = dbeta, geom = "polygon", colour = pal[2],
                fill = pal[2], alpha = 0.75, args=list(shape1=20, shape2=20), show.legend=F)+
  stat_function(fun = dbeta, geom = "polygon", colour = pal[3],
                fill = pal[3], alpha = 0.75, args=list(shape1=40, shape2=10), show.legend=F)+
  stat_function(fun = dbeta, geom = "polygon", colour = pal[4],
                fill = pal[4], alpha = 0.75, args=list(shape1=20, shape2=80), show.legend=F)+
  stat_function(fun = dbeta, geom = "polygon", colour = pal[5],
                fill = pal[5], alpha = 0.75, args=list(shape1=2, shape2=8), show.legend=F)+
  stat_function(fun = dbeta, geom = "polygon", colour = pal[6],
                fill = pal[6], alpha = 0.75,args=list(shape1=100, shape2=10), show.legend=F)+
  pcv_theme()

beta_dist_plot

#* ***** `Beta Binomial` *****

values <- c(unlist(lapply(1:100, function(i) {set.seed(i);rbinom(10, 100, prob = rbeta(1, 2,2) )})),
  unlist(lapply(1:100, function(i) {set.seed(i);rbinom(10, 100, prob = rbeta(1, 20,20) )})),
  unlist(lapply(1:100, function(i) {set.seed(i);rbinom(10, 100, prob = rbeta(1, 40,10) )})),
  unlist(lapply(1:100, function(i) {set.seed(i);rbinom(10, 100, prob = rbeta(1, 20,80) )})),
  unlist(lapply(1:100, function(i) {set.seed(i);rbinom(10, 100, prob = rbeta(1, 2,8) )})),
  unlist(lapply(1:100, function(i) {set.seed(i);rbinom(10, 100, prob = rbeta(1, 100,10) )})) )
Beta = factor(c(rep(c("B(2,2)", "B(20,20)", "B(40,10)", "B(20,80)", "B(2,8)", "B(100,10)"), each = 1000)),
              levels=c("B(2,2)", "B(20,20)", "B(40,10)", "B(20,80)", "B(2,8)", "B(100,10)"))

bndf<-data.frame(values, Beta)

betaBinom_dist_plot<-ggplot(bndf, aes(x = values, fill=Beta))+
  geom_histogram(alpha=0.7, position="identity", binwidth=1)+
  scale_fill_viridis_d()+
  guides(fill=guide_legend(override.aes = list(alpha=1)))+
  labs(x="Successes from 100 Trials", y = "Count", fill="Beta", title="Beta Binomial Conjugacy")+pcv_theme()

layout<-c(area(1,1,3,4), area(4,1,4,4))

conjugacy_ex <- betaBinom_dist_plot / beta_dist_plot + plot_layout(design = layout, guides="collect")

```

## Conjugacy Example

```{r}
#| echo: false
#| eval: true
conjugacy_ex
```

## `conjugate` function

`conjugate` uses conjugate priors and moments from given distributions to make computationally light Bayesian comparisons of single or multi value traits.

Several methods are available for different distributions.

## `conjugate` function (2) {.smaller}

-   t: Difference of means for gaussian data.
-   gaussian: Difference of distributions for gaussian data.
-   beta: Difference of distributions for data bounded \[0,1\]
-   lognormal: Difference of distributions for log normal data (right skew)
-   poisson: Difference of distributions for count data
-   negative binomial: Alternative difference of distributions for count data (given "r")
-   dirichlet: Difference of categorical distributions (2 versions)

## `conjugate` function (3)

```{r}
#| echo: false
#| output-location: slide
library(extraDistr) # also should be covered by pcvr
methods<-c("lognormal","t","gaussian","beta","poisson","negbin")
p1 <- ggplot(data.frame(x=seq(0, 250, 0.1),
                        fill=factor("lognormal", levels = methods)),
             aes(x=x, fill=fill))+
  stat_function(geom="polygon", fun = dlnorm, args = list(meanlog=log(70), sdlog=log(1.5)))+
  scale_fill_manual(values=viridis::viridis(6,1, 0.1, 0.9), drop = F)+
  pcv_theme()+
  labs(title="lognormal")

p2 <- ggplot(data.frame(x=seq(0, 20, 0.01), 
                        fill=factor("t", levels =methods)),
             aes(x=x, fill=fill))+
  stat_function(geom="polygon", fun = dlst, args = list(df = 3, mu = 10, sigma = 2))+
  scale_fill_manual(values=viridis::viridis(6,1, 0.1, 0.9), drop = F)+
  pcv_theme()+
  labs(title="T")

p3 <- ggplot(data.frame(x=seq(0, 20, 0.001),
                        fill=factor("gaussian", levels = methods)),
             aes(x=x, fill=fill))+
  stat_function(geom="polygon", fun = dnorm, args = list(mean = 10, sd = 2))+
  scale_fill_manual(values=viridis::viridis(6,1, 0.1, 0.9), drop = F)+
  pcv_theme()+
  labs(title="gaussian")

p4 <- ggplot(data.frame(x=seq(0, 1, 0.0001),
                        fill=factor("beta", levels = methods)),
             aes(x=x, fill=fill))+
  stat_function(geom="polygon", fun = dbeta, args = list(shape1=10, shape2=5))+
  scale_fill_manual(values=viridis::viridis(6,1, 0.1, 0.9), drop = F)+
  pcv_theme()+
  labs(title="beta")

set.seed(123)
pois<-hist(rpois(100000, 12), breaks=seq(0,30,1), plot=F)
p5 <- ggplot(data.frame(x=pois$breaks[-31], y=pois$counts, 
                        fill=factor("poisson",levels = methods)),
             aes(x=x, y=y, fill=fill))+
  geom_col()+
  scale_fill_manual(values=viridis::viridis(6,1, 0.1, 0.9), drop = F)+
  pcv_theme()+
  labs(title="poisson")

set.seed(123)
nb<-hist(rnbinom(100000, 5, 0.2), breaks=seq(0,100,1), plot=F)
p6 <- ggplot(data.frame(x=nb$breaks[-101], y=nb$counts,
                        fill=factor("negbin", levels = methods)),
             aes(x=x, y=y, fill=fill))+
  geom_col()+
  scale_fill_manual(values=viridis::viridis(6,1, 0.1, 0.9), drop = F)+
  pcv_theme()+
  labs(title="negbin")
library(patchwork)
(p1+p2+p3)/(p4+p5+p6)+plot_layout(guides="collect")
```

## Bayesian comparison with `conjugate`

Here is an example of using the `conjugate` function from `pcvr` to do a Bayesian analog to a T test and a region of practical equivalence (*ROPE*) comparison.

```{r}
#| eval: false
#| echo: true
s1 <- sv_ag[sv_ag$fertilizer=='100' & sv_ag$DAS==19 & sv_ag$genotype=="W605S", "area_cm2"]
s2 <- sv_ag[sv_ag$fertilizer=='50' & sv_ag$DAS==19 & sv_ag$genotype=="W605S", "area_cm2"]
conj<-conjugate(s1, s2, method="t", plot=T,
                priors = list( mu=c(20,20),n=c(1,1),s2=c(20,20) ),
                rope_range=c(-10,10),
                support = seq(0,125,0.1))
```

## Bayesian comparison with `conjugate`

```{r}
#| echo: false
s1 <- sv_ag[sv_ag$fertilizer=='100' & sv_ag$DAS==19 & sv_ag$genotype=="W605S", "area_cm2"]
s2 <- sv_ag[sv_ag$fertilizer=='50' & sv_ag$DAS==19 & sv_ag$genotype=="W605S", "area_cm2"]
conj<-conjugate(s1, s2, method="t", plot=T,
                priors = list( mu=c(20,20),n=c(1,1),s2=c(20,20) ),
                rope_range=c(-10,10), support = seq(0,125,0.1))
```

## Bayesian comparison with `conjugate` (2)

`conjugate` returns a list with several useful components.

```{r}
#| echo: true
lapply(conj, class)#[1:length(conj)]
```

## Bayesian comparison with `conjugate` (2)

The summary item contains HDE and HDI estimates for samples and their difference (if two samples are provided).

```{r}
#| echo: true
conj$summary
```

The posterior item is an updated set of priors. It can be recycled as more data is collected in Bayesian updating, but the practical speed difference is trivial.

The remaining items are the plot and data that was used in making the plot.

## [conjugate (exercise)]{style="color:#FFD900;"}

Convert height from px to cm and report the probability that the difference in height between B73 in 50% fertilizer and W605x in 0% fertilizer is larger than $|10\text{cm}|$.

## [conjugate (solution)]{style="color:#FF007B;"}

```{r}
#| echo: true
s1 <- sv_ag[sv_ag$fertilizer=='50' & sv_ag$DAS==19 &
              sv_ag$genotype=="B73", "height_pixels"]/42.5
s2 <- sv_ag[sv_ag$fertilizer=='0' & sv_ag$DAS==19 &
              sv_ag$genotype=="W605S", "height_pixels"]/42.5

conj<-conjugate(s1,s2, method="gaussian", plot=T,
                priors = list( mu=c(20,20),n=c(1,1),s2=c(20,20) ),
                support = seq(0,60,0.1), rope_range = c(-10,10))
# 1-conj$summary$rope_prob: 0.318
```

## Longitudinal Analysis

One of the core benefits of image based high throughput phenotyping is that you can quickly collect non-destructive samples.

Keeping the plants alive allows for longitudinal data on individuals.

## Longitudinal Analysis Challenges

These benefits add complexity to our data.

Longitudinal data is:

-   Autocorrelated
-   Often non-linear
-   Heteroskedastic

## Modeling longitudinal data

`pcvr` supports 8 growth models fit with 4 different methods (`nls`, `nlrq`, `nlme`, and `brms`).

## Modeling longitudinal data

```{r}
#| echo: false
#| eval: true
simdf<-growthSim("logistic", n=20, t=25, params = list("A"=c(200,160), "B"=c(13, 11), "C"=c(3, 3.5)))
l<-ggplot(simdf,aes(time, y, group=interaction(group,id)))+
  geom_line(aes(color=group))+labs(title="Logistic")+theme_minimal()+theme(legend.position="none")

simdf<-growthSim("gompertz", n=20, t=25, params = list("A"=c(200,160), "B"=c(13, 11), "C"=c(0.2, 0.25)))
g<-ggplot(simdf,aes(time, y, group=interaction(group,id)))+
  geom_line(aes(color=group))+labs(title="Gompertz")+theme_minimal()+theme(legend.position="none")

simdf<-growthSim("monomolecular", n=20, t=25, params = list("A"=c(200,160), "B"=c(0.08, 0.1)))
m<-ggplot(simdf,aes(time, y, group=interaction(group,id)))+
  geom_line(aes(color=group))+labs(title="Monomolecular")+theme_minimal()+theme(legend.position="none")

simdf<-growthSim("exponential", n=20, t=25, params = list("A"=c(15, 20), "B"=c(0.095, 0.095)))
e<-ggplot(simdf,aes(time, y, group=interaction(group,id)))+
  geom_line(aes(color=group))+labs(title="Exponential")+theme_minimal()+theme(legend.position="none")

simdf<-growthSim("linear", n=20, t=25, params = list("A"=c(1.1, 0.95)))
ln<-ggplot(simdf,aes(time, y, group=interaction(group,id)))+
  geom_line(aes(color=group))+labs(title="Linear")+theme_minimal()+theme(legend.position="none")

simdf<-growthSim("power law", n=20, t=25, params = list("A"=c(16, 11), "B"=c(0.75, 0.7)))
pl<-ggplot(simdf,aes(time, y, group=interaction(group,id)))+ geom_line(aes(color=group))+
  labs(title="Power Law")+theme_minimal()+theme(legend.position="none")

simdf<-growthSim("double logistic", n=20, t=70,
                 params = list("A"=c(70,50), "B"=c(20, 15), "C"=c(3, 3),
                               "A2"=c(160,210), "B2"=c(45, 45), "C2"=c(4,4)))
dl <- ggplot(simdf,aes(time, y, group=interaction(group,id)))+
 geom_line(aes(color=group))+labs(title="Double Logistic")+
  theme_minimal()+
  theme(legend.position = "none")

simdf<-growthSim("double gompertz", n=20, t=70,
                 params = list("A"=c(70,50), "B"=c(8, 8), "C"=c(0.2, 0.2),
                               "A2"=c(160,210), "B2"=c(35, 40), "C2"=c(0.1, 0.1)))
dg <- ggplot(simdf,aes(time, y, group=interaction(group,id)))+
 geom_line(aes(color=group))+labs(title="Double Gompertz")+
  scale_y_continuous(limits = layer_scales(dl)$y$range$range)+
  theme_minimal()+
  theme(legend.position = "none",
        axis.text.y=element_blank(),
        axis.title.y=element_blank())

layout = c(area(1,1,1,1), # top, left, bottom , right
           area(1,2,1,2),
           area(1,3,1,3),
           area(1,4,1,4),
           area(2,1,2,1), 
           area(2,2,2,2),
           area(2,3,2,3),
           area(2,4,2,4))

patch8<-(l+g+m+dg+e+ln+pl+dl)+
  plot_layout(guides="collect", design=layout)
patch8
```

## Data preparation

For this example we subset the data, but we could also model the entire dataset by making a new `group` column that has fertilizer and genotype information.

```{r}
#| echo: true
sub<-sv_ag[sv_ag$genotype=="W605S",]
sub$fertilizer=as.character(sub$fertilizer)
```

To set up our model it generally helps to plot the data and make sure we can use an appropriate model and can identify our groups and individuals.

```{r}
#| echo: true
#| output-location: slide
ggplot(sub, aes(x= DAS, y=area_cm2, group=barcode, color=fertilizer))+
  geom_line()+
  theme_minimal()
```

## Initializing with `growthSS`

The `growthSS` function serves as a "self starter" for 8 common growth models (logistic, gompertz, monomolecular, linear, exponential, and power law).

```{r}
#| echo: true
#| eval: false
ss<-growthSS(model="gompertz",
             form =  area_cm2~DAS|barcode/fertilizer,
             sigma="spline",
             df=sub,
             priors = list("A" = 130, "B" = 15, "C" = 3),
             type="brms")
```

See `?growthSS` and `?growthSim` for details.

## Fitting with `fitGrowth`

If you are using one of the 6 common growth models supported by `growthSS` then you can use `fitGrowth` to fit the model using Stan.

```{r}
#| eval: false
#| echo: true
fit_test <- fitGrowth(ss, iter = 1000,
                      cores = 2, chains = 2, backend = "cmdstanr",
               control = list(adapt_delta = 0.999, max_treedepth = 20))
```

```{r}
#| eval: false
#| echo: true
summary(fit_test)
```

## Plotting with `growthPlot`

Any of the 4 types of growth models can be plotted using `growthPlot`.

![](patched_growthPlots.png)

## Testing with `testGrowth`

This example uses a Bayesian model where hypothesis testing is much more robust.

For examples of `testGrowth` on frequentist models see the intermediate growth modeling tutorial.

## Resources

For a more complete workflow using a small bellwether dataset see the `bellwether` vignette and [tutorials](https://github.com/danforthcenter/pcvr/tree/main/tutorials) on github.

```{r}
#| eval: false
#| echo: true
browseVignettes("pcvr") # requires you to have build vignettes on installation
# see installation document
```

The `help-datascience` DDPSC slack channel and `pcvr` [github repository](https://github.com/danforthcenter/pcvr) are good places to ask questions.
